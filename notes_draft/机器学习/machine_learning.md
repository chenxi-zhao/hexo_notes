### 机器学习概述
机器学习就是把无序的数据转换成有用的信息。
1. 获取海量的数据
2. 从海量数据中获取有用的信息

>我们会利用计算机来彰显数据背后的真实含义，这才是机器学习的意义。
>[模式识别、机器学习、和深度学习](https://www.csdn.net/article/2015-03-24/2824301)


### 机器学习的组成
#### 主要任务
- 分类：将实例数据划分到合适的类别中。
- 回归：主要用于预测数值型数据。（示例：数据通过给定数据点来拟合最优曲线）

#### 监督学习
- 必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。 (包括：分类和回归)
- 样本集：训练数据 + 测试数据
    1. 训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)
    2. 特征通常是训练样本集的列，它们是独立测量得到的。
    3. 目标变量: 目标变量是机器学习预测算法的测试结果。在分类算法中目标变量的类型通常是标称型(如：真与假)，而在回归算法中通常是连续型(如：1~100)。
- 知识表示：
    1. 可以采用规则集的形式【例如：数学成绩大于90分为优秀】
    2. 可以采用概率分布的形式【例如：通过统计分布发现，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】
    3. 可以使用训练样本集中的一个实例【例如：通过样本集合，我们训练出一个模型实例，得出 年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】

#### 非监督学习
- 数据没有类别信息，也不会给定目标值。
- 聚类：在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。
- 密度估计：将寻找描述数据统计值的过程称之为密度估计。【就是：根据训练样本确定x的概率分布】
- 此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。

#### 用于执行分类、回归、聚类和密度的主要算法
- 监督学习：
    1. k-近邻算法： 线性回归
    2. 朴素贝叶斯： 局部加权线性回归
    3. 支持向量机： Ridge回归
    4. 决策树： Lasso最小回归系数估计
- 无监督学习：
    1. k-均值： 最大期望
    2. DBSCAN： Parzen窗设计

#### 示例
1. 算法场景
    - 预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法
    - 给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。
2. 需要收集或分析的数据是什么

![](https://raw.githubusercontent.com/apachecn/MachineLearning/master/images/1.MLFoundation/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95.jpg)

##### 机器学习 开发流程
- 收集数据: 收集样本数据
- 准备数据: 注意数据的格式
- 分析数据: 为了确保数据集中没有垃圾数据；
    - 如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；
    - 另外该步骤需要人工干预，会降低自动化系统的价值。
- 训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤
- 测试算法: [机器学习算法核心]评估算法效果
- 使用算法: 将机器学习算法转为应用程序



### K-近邻
k-近邻（kNN, k-NearestNeighbor）算法是一种基本分类与回归方法。

>k-近邻算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。
>k-近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k-个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k-近邻算法不具有显式的学习过程。
>**k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。**

#### 原理
1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。
2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。
    1. 计算新数据与样本数据集中每条数据的距离。
    2. 对求得的所有距离进行排序（从小到大，越小表示越相似）。
    3. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。
3. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。

#### 特点
- 优点：精度高、对异常值不敏感、无数据输入假定
- 缺点：计算复杂度高、空间复杂度高

适用数据范围：数值型和标称型